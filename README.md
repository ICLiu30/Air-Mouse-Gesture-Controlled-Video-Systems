# Air-Mouse-Gesture-Controlled-Video-Systems

## Demos

### Air Mouse Functionality:
In constructing the Air Mouse system, a combination of tools including MediaPipe, OpenCV, and PyAutoGUI were employed. With these technologies at its core, the system adeptly recognizes and responds to finger movements, making actions like scrolling, clicking, and cursor navigation fluid and intuitive. Beyond these traditional functions, the inclusion of speech recognition technology also empowers users to articulate words, which the system then dutifully types out, minimizing the need for manual text input.
![Mouse Functionality Demo](GIF/Mouse.gif)

### Gesture-Controlled Video:
For the Gesture-Controlled Video aspect, the foundation lies in the capabilities of the Random Forest model. This model has been trained to discern various hand gestures, effectively translating them into actionable video commands. From fast-forwarding and rewinding to initiating a pause or adjusting the volume, the system comprehends a user's every gesture, optimizing the video-watching experience.
![Gesture-Controlled Video Demo](GIF/Video.gif)

## Files Description

- **GIF**: Contains demo GIFs showcasing the functionality of the system.
- **Create&Train.ipynb**: Jupyter notebook detailing the data creation and training process.
- **module.py**: The primary module containing essential functions utilized by the system.
- **app.py**: The main application script driving the system.
- **requirements.txt**: Lists all the dependencies required to run the project.
- **web.py**: (Description for web.py goes here.)

## Setup & Execution

1. Clone the repository.
2. Install required packages using pip install -r requirements.txt
3. Run app.py to launch the application.

## Acknowledgements
Special thanks to [Murtaza's Workshop](https://www.youtube.com/watch?v=8gPONnGIPgw) for their guidance during this project's development.

For a deeper dive into the code and training, explore this repository. Feel free to raise any issues or contribute!
